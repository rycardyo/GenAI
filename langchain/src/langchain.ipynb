{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      3\u001b[39m llm_model = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m chat = ChatOpenAI(temperature = \u001b[32m0\u001b[39m, \n\u001b[32m      6\u001b[39m                   model = llm_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\DS\\genAI\\genAiEnv\\Lib\\site-packages\\langchain\\chat_models\\__init__.py:29\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chat_models\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_model = ''\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0, \n",
    "                  model = llm_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Trauza o texto delimitado por \"\",\n",
      "    para o estilo  Vegetta muito furioso\n",
      "    \" \n",
      "    Voce nao poderia me derrotar, pois por ventura, sou deveras\n",
      "    mais poderoso que ti hihhihi, agora contemple minha tecnica secreta: \n",
      "\", também complete a técnica secreta de acordo com o personagem \n",
      "     citado no style \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "#text = ''\n",
    "#style = 'O mesmo estilo de fala que o batman teria, o tom de batman sério'\n",
    "template_string = \"\"\"\n",
    "    Trauza o texto delimitado por \"\",\n",
    "    para o estilo {style}\n",
    "    \"{text}\", também complete a técnica secreta de acordo com o personagem \n",
    "     citado no style \"\"\"\n",
    "\n",
    "text = ''' \n",
    "    Voce nao poderia me derrotar, pois por ventura, sou deveras\n",
    "    mais poderoso que ti hihhihi, agora contemple minha tecnica secreta: \n",
    "'''\n",
    "\n",
    "style = ''' Vegetta muito furioso'''\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "formatPrompt = prompt_template.format_messages(\n",
    "    style = style, \n",
    "    text = text \n",
    ")\n",
    "\n",
    "print(formatPrompt[0].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='\\n    Trauza o texto delimitado por \"\",\\n    para o estilo {style}\\n    \"{text}\", também complete a técnica secreta de acordo com o personagem \\n     citado no style ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "A way that let you define how python can be deal with the ouput of model. \n",
    "\n",
    "Example: I want that my output have a json format... returns needs to be a python dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gonna be create a parser to a json format, with the keys:\n",
    "\n",
    "    * gift:\n",
    "    * deleivery_days\n",
    "    * prince_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one schema for each key in the output\n",
    "# O schema description é puramente documental, ou possui \n",
    "# alguma finalidade de orientações ao modelo?\n",
    "\n",
    "gift_schema = ResponseSchema(name = 'gift',\n",
    "                             description= (\"Was the item purchased as a gift for someone else?\"\n",
    "                                            \"response True for yse, \"\n",
    "                                            \"False if not or unknown\")\n",
    "                                            )\n",
    "\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name = 'delivery_days',\n",
    "    description= (\"How many days didi it take for the product to arrive?\"\n",
    "                  \"If this information is not found, output -1\")\n",
    "    )\n",
    "\n",
    "\n",
    "price_value_schema = ResponseSchema(\n",
    "    name = 'price_value',\n",
    "    description = 'Extract any sentences about the value or price, and out them as a comma separated Python list'\n",
    ")\n",
    "\n",
    "responseSchemas = [gift_schema, delivery_days_schema, price_value_schema]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_schemas=[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else?response True for yse, False if not or unknown', type='string'), ResponseSchema(name='delivery_days', description='How many days didi it take for the product to arrive?If this information is not found, output -1', type='string'), ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and out them as a comma separated Python list', type='string')]\n"
     ]
    }
   ],
   "source": [
    "# Cria uma \"tupla\" com \"n\" elementos, um para cada schema/key definido.\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(responseSchemas)\n",
    "print(output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased as a gift for someone else?response True for yse, False if not or unknown\n",
      "\t\"delivery_days\": string  // How many days didi it take for the product to arrive?If this information is not found, output -1\n",
      "\t\"price_value\": string  // Extract any sentences about the value or price, and out them as a comma separated Python list\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Im a template string with the following paramters:\n",
      "\n",
      "    text: blablabla\n",
      "\n",
      "    The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased as a gift for someone else?response True for yse, False if not or unknown\n",
      "\t\"delivery_days\": string  // How many days didi it take for the product to arrive?If this information is not found, output -1\n",
      "\t\"price_value\": string  // Extract any sentences about the value or price, and out them as a comma separated Python list\n",
      "}\n",
      "```\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "customer_review = 'blablabla'\n",
    "template_string = '''\n",
    "    Im a template string with the following paramters:\n",
    "    \n",
    "    text: {text}\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    " '''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template = template_string)\n",
    "messages = prompt.format_messages(\n",
    "    text = customer_review,\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocar o modelo com o parser...\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apos o modelo processar o conteudo, utilizar o parser para a atividade de parsing do output\n",
    "output_parsed = output_parser.parse(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory for chat bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Tudo bem por aqui! 😊 E com você? Como posso te ajudar hoje?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []}, id='run--10e169c4-c111-41fd-b698-81a402b1dbc2-0', usage_metadata={'input_tokens': 3, 'output_tokens': 17, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai     import ChatGoogleGenerativeAI \n",
    "from langchain.chains           import ConversationChain\n",
    "from langchain.memory           import ConversationBufferMemory \n",
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "load_dotenv()\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash-lite-001')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricar\\AppData\\Local\\Temp\\ipykernel_3184\\3770664563.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\ricar\\AppData\\Local\\Temp\\ipykernel_3184\\3770664563.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chatBot = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "chatBot = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory, \n",
    "    verbose = True #If true, tracking the llm execution pipeline \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi my name is Ricardo\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ricardo! It's a pleasure to meet you. My name is Bard, and I'm a large language model, trained by Google. I'm designed to be informative and comprehensive. I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I try my best to follow your instructions and complete your requests thoughtfully. I'm ready when you are! What can I help you with today, Ricardo? Do you have any questions, or are you looking for something specific? Perhaps you'd like me to tell you a joke, write a short story, or maybe even translate something? Just let me know!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBot.predict(input=\"hi my name is Ricardo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi my name is Ricardo\n",
      "AI: Hello Ricardo! It's a pleasure to meet you. My name is Bard, and I'm a large language model, trained by Google. I'm designed to be informative and comprehensive. I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I try my best to follow your instructions and complete your requests thoughtfully. I'm ready when you are! What can I help you with today, Ricardo? Do you have any questions, or are you looking for something specific? Perhaps you'd like me to tell you a joke, write a short story, or maybe even translate something? Just let me know!\n",
      "Human: How many r´s are in strawberry?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question, Ricardo! Let me think... Hmmm... Okay, so the word \"strawberry\" is spelled S-T-R-A-W-B-E-double R-Y. Therefore, there are two \"R\"s in the word \"strawberry\". Is there anything else I can help you with today? Perhaps you'd like to know the etymology of the word \"strawberry\"? Did you know that strawberries aren't actually berries in a botanical sense? Fascinating, right?\n"
     ]
    }
   ],
   "source": [
    "print( chatBot.predict(input = 'How many r´s are in strawberry?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi my name is Ricardo\n",
      "AI: Hello Ricardo! It's a pleasure to meet you. My name is Bard, and I'm a large language model, trained by Google. I'm designed to be informative and comprehensive. I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I try my best to follow your instructions and complete your requests thoughtfully. I'm ready when you are! What can I help you with today, Ricardo? Do you have any questions, or are you looking for something specific? Perhaps you'd like me to tell you a joke, write a short story, or maybe even translate something? Just let me know!\n",
      "Human: How many r´s are in strawberry?\n",
      "AI: That's a great question, Ricardo! Let me think... Hmmm... Okay, so the word \"strawberry\" is spelled S-T-R-A-W-B-E-double R-Y. Therefore, there are two \"R\"s in the word \"strawberry\". Is there anything else I can help you with today? Perhaps you'd like to know the etymology of the word \"strawberry\"? Did you know that strawberries aren't actually berries in a botanical sense? Fascinating, right?\n",
      "Human: Whats is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Ricardo! I remember you told me that at the beginning of our conversation. Is there anything else I can help you with, Ricardo? Perhaps you'd like me to write a short poem about strawberries, since we were just talking about them? Or maybe you'd like to know some interesting facts about other fruits? I'm ready when you are!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBot.predict(input = 'Whats is my name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricar\\AppData\\Local\\Temp\\ipykernel_3184\\3143180504.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "# Definindo o quao \"larga\" sera a janela de contexto (em numero de past messages)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "chatBot = ConversationChain(\n",
    "    llm = llm , \n",
    "    memory = memory, \n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My name is Ricardo\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Ricardo! It's a pleasure to meet you. I'm an AI, a large language model trained by Google. I don't have a name, but you can call me anything you like! It's nice to be chatting with you today. Is there anything I can help you with? Perhaps you have a question about something, or maybe you just want to chat? I'm ready for whatever you have in mind! I can access and process information from the real world through Google Search and keep my response consistent with search results. So, just let me know what's on your mind!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBot.predict(input = 'My name is Ricardo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Ricardo\n",
      "AI: Hello Ricardo! It's a pleasure to meet you. I'm an AI, a large language model trained by Google. I don't have a name, but you can call me anything you like! It's nice to be chatting with you today. Is there anything I can help you with? Perhaps you have a question about something, or maybe you just want to chat? I'm ready for whatever you have in mind! I can access and process information from the real world through Google Search and keep my response consistent with search results. So, just let me know what's on your mind!\n",
      "Human: How many r´s are in strawberry\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That\\'s a fun question, Ricardo! Let\\'s find out together. Based on my knowledge, which includes information from Google Search, the word \"strawberry\" has **three** \"r\"s. '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBot.predict(input = 'How many r´s are in strawberry')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: How many r´s are in strawberry\n",
      "AI: That's a fun question, Ricardo! Let's find out together. Based on my knowledge, which includes information from Google Search, the word \"strawberry\" has **three** \"r\"s. \n",
      "Human: Whats my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I do not have access to your name, Ricardo. My knowledge about you is limited to what you share in our conversation.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBot.predict(input = 'Whats my name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAiEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
